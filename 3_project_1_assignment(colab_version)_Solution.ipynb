{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodandachalla/Data_Analysis_and_Visualization__Music/blob/main/3_project_1_assignment(colab_version)_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EB4ftsyhBDk"
      },
      "source": [
        "***N.B. this notebook has been tested in Google Colab***\n",
        "\n",
        "# Music Genre Classification\n",
        "\n",
        "In this project, you will build a machine learning algorithm to classify music genres using audio features. Starting with the provided dataset, your task is to develop a model that effectively solves this multiclass classification problem. Use the baseline notebook as a starting point and improve upon it.\n",
        "\n",
        "***Overall Goal: to design a complete pipeline that improves accuracy from the current 37% to at least 70%, ideally approaching 80%***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTVG0TRmazpB"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this project, you will work with a dataset of music samples from various genres. The dataset has been purposely left a bit messy, with some entries missing labels and others containing empty audio files. To start with, your task is to clean and explore the dataset, turning it into a well-organized resource for analysis.\n",
        "\n",
        "This notebook includes a basic, \"weak\" baseline to get you started. It serves as a simple starting point, but it is neither thorough nor accurate. You are expected to build upon it, applying your own strategies to improve the data science pipeline (including data cleaning, curation, feature engineering, etc) before moving into model building, parameters tuning, and model evaluation.\n",
        "\n",
        "The formal details of the assignment are provided at the end of the notebook. To start with, focus on understanding the dataset and planning your strategies to tackle its challenges.\n",
        "\n",
        "**We expect you to submit a modified version of this notebook with your improvements. Please download a copy of this assignment in your private Python programming environment, before making any changes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqFo3t-bix6"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jBR6dB4g01p"
      },
      "source": [
        "Let's install all required dependencies:\n",
        "\n",
        "- **datasets**: Access to large-scale datasets.\n",
        "- **librosa**: Tools for audio analysis.\n",
        "- **pandas** & **numpy**: Tabular data manipulation and numerical operations.\n",
        "- **scikit-learn**: Machine learning algorithms and tools.\n",
        "- **tqdm**: Progress bar.\n",
        "\n",
        "You might be familiar with most of these already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmucrCDx6aZN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==3.5.0 librosa pandas numpy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5QDV6Wig85e"
      },
      "source": [
        "And import the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKYw6B3g6pPm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from datasets import load_dataset\n",
        "from IPython.display import Audio, display\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKxZKHnjeM6l"
      },
      "outputs": [],
      "source": [
        "# Include here any additional modules that you might need\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCFUq7TyWUT3"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The dataset consists of music samples from various genres, including:\n",
        "\n",
        "- **Genres**: `Blues`, `Classical`, `Country`, `Disco`, `HipHop`, `Jazz`, `Metal`, `Pop`, `Reggae`, and `Rock`.\n",
        "\n",
        "The dataset is a bit messy and includes some **unlabeled data** and **empty audio files**. We have provided basic preprocessing, but more in-depth data cleaning, feature extraction, and preparation will be a part of your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw0SVWUYhLBO"
      },
      "source": [
        "Let's download the audio dataset using the Hugging Face datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzKXSQs36qV4"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"unibz-ds-course/audio_assignment\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXY6iDBWwRwN"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aji8xId4qFe"
      },
      "outputs": [],
      "source": [
        "print(f\"Num of samples in the dataset: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ajqJ4-ZVvq"
      },
      "source": [
        "Let's take a glance at a sample from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4um4LfjAa0l3"
      },
      "outputs": [],
      "source": [
        "entry = dataset[10]\n",
        "\n",
        "audio_array = entry['audio']['array']\n",
        "sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "print(f\"Element: {entry}\\n\")\n",
        "print(f\"File Path: {entry['file']}\")\n",
        "print(f\"Number of Samples: {len(audio_array)}\")\n",
        "print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
        "\n",
        "audio_length_seconds = len(audio_array) / sampling_rate\n",
        "print(f\"Audio Length: {audio_length_seconds:.2f} seconds\")\n",
        "\n",
        "genre_id = entry['genre']\n",
        "genre_label = dataset.features['genre'].int2str(genre_id)\n",
        "print(f\"Genre (ID): {genre_id}\")\n",
        "print(f\"Genre (Label): {genre_label}\")\n",
        "\n",
        "display(Audio(audio_array, rate=sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qW7b3HLeoEq"
      },
      "source": [
        "**Draw a plot with distribution of the classes (5 points)**\n",
        "\n",
        "Create a visualization (e.g., bar chart or histogram) that shows how many samples belong to each class.\n",
        "This helps identify whether the dataset is balanced or if some classes are underrepresented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQeAMy9l2w7l"
      },
      "outputs": [],
      "source": [
        "genre_counts = pd.Series(dataset['genre']).value_counts().sort_index()\n",
        "genre_labels = [dataset.features['genre'].int2str(int(g_id)) for g_id in genre_counts.index]\n",
        "\n",
        "display(\"The dataset contains balanced classes across different genres\")\n",
        "plt.figure(figsize=(15, 6))\n",
        "ax = sns.barplot(x=genre_labels, y=genre_counts.values, palette='tab10', hue=genre_labels, legend=False)\n",
        "plt.title('Distribution of Music Genres in the Dataset')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=0, ha='right')\n",
        "\n",
        "total_samples = len(dataset)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    percentage = '{:.1f}%'.format(100 * height / total_samples)\n",
        "    ax.annotate(percentage,\n",
        "                (p.get_x() + p.get_width() / 2., height),\n",
        "                ha='center', va='bottom', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQSbN8NXe5c7"
      },
      "source": [
        "**Draw distribution of lengths of audios (5 points)**\n",
        "\n",
        "Plot the distribution of audio lengths in the dataset to analyze how durations vary across samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6ufj2m2s-O"
      },
      "outputs": [],
      "source": [
        "audio_array_lengths = [len(sample['audio']['array']) for sample in dataset]\n",
        "sampling_rates = [sample['audio']['sampling_rate'] for sample in dataset]\n",
        "\n",
        "audio_length_seconds = [length / sampling_rate for length, sampling_rate in zip(audio_array_lengths, sampling_rates)]\n",
        "\n",
        "print(f\"Minium Audio Length: {min(audio_length_seconds):.2f} seconds\")\n",
        "print(f\"Maximum Audio Length: {max(audio_length_seconds):.2f} seconds\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(audio_length_seconds, bins=50, kde=True)\n",
        "plt.title('Distribution of Audio Lengths')\n",
        "plt.xlabel('Audio Length (seconds)')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrhiNmjPglH4"
      },
      "source": [
        "**Delete empty samples (5 points)**\n",
        "\n",
        "Implement the function to remove empty samples (audios with silence only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inj3l4oU_nFq"
      },
      "outputs": [],
      "source": [
        "def filter_empty_samples(entry):\n",
        "    audio_array = entry['audio']['array']\n",
        "\n",
        "    # Check if the audio array is truly empty (length 0)\n",
        "    if len(audio_array) == 0:\n",
        "        return False\n",
        "    # Check if all elements in the audio array are zero (silence only)\n",
        "    if np.all(audio_array == 0):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf73y3TMhJK7"
      },
      "outputs": [],
      "source": [
        "filtered_dataset = dataset.filter(filter_empty_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmf3nwvzmiUn"
      },
      "outputs": [],
      "source": [
        "assert len(filtered_dataset) == 970, \"Your filtering function is wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5HPxWYoJHqI"
      },
      "source": [
        "Uncomment the code above. If the assertion fails, please check your function for bugs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCMjE5UYlRPu"
      },
      "source": [
        "**Delete unlabeled samples (5 points)**\n",
        "\n",
        "Implement the function to remove unlabeled samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ROox7StlV6N"
      },
      "outputs": [],
      "source": [
        "def filter_unlabeled_samples(entry):\n",
        "    genre_id = entry['genre']\n",
        "\n",
        "    if genre_id not in genre_counts.index: # 0 to 9\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpGekQyDlk5o"
      },
      "outputs": [],
      "source": [
        "filtered_dataset = filtered_dataset.filter(filter_unlabeled_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3abNWE7glwI4"
      },
      "outputs": [],
      "source": [
        "assert len(filtered_dataset) == 848, \"Your filtering function is wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuK76xWTJSFh"
      },
      "source": [
        "Uncomment the code above. If the assertion fails, please check your function for bugs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2-AKqdvv-Z"
      },
      "source": [
        "Now we can extract some features from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmfDsReXmFCE"
      },
      "source": [
        "### **Mel Frequency Cepstral Coefficients**\n",
        "\n",
        "**[Mel Frequency Cepstral Coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)** are commonly used in audio analysis to capture key features of sound. They help represent the important characteristics of an audio signal, making them ideal for tasks like music genre classification and speech recognition.\n",
        "\n",
        "We're not going to dive deep into the complex details of audio processing, but it's useful to know that MFCCs help simplify raw audio data while retaining important information.\n",
        "\n",
        "#### Basic Steps in MFCC Extraction:\n",
        "1. **Frequency Domain Conversion**: The audio signal is split into short frames, and we apply the Fourier Transform to convert them from the time domain to the frequency domain.\n",
        "2. **Mel Scale Mapping**: The frequency spectrum is converted to the Mel scale, which better represents how humans perceive sound, emphasizing lower frequencies.\n",
        "3. **Logarithm and DCT**: After mapping to the Mel scale, we apply a logarithm and the Discrete Cosine Transform (DCT) to get the MFCCs. These summarize the \"cepstral\" information of the audio signal.\n",
        "\n",
        "The parameter `n_mfcc` controls **how many MFCC coefficients** are extracted for each frame. For example, setting `n_mfcc=8` means we extract 8 coefficients, where lower coefficients capture broad audio features, and higher coefficients capture the more finer details.\n",
        "\n",
        "#### Why MFCCs Are Important:\n",
        "MFCCs help capture the **tonal quality** of the sound and reduce the complexity of the raw audio signal. By summarizing the audio into a smaller set of features, they allow machine learning models to classify and recognize different types of sounds more effectively.\n",
        "\n",
        "In this notebook, we'll use the **mean** and **variance** of the MFCCs over time to create a robust feature set for our classification model. Adjusting the `n_mfcc` parameter allows us to control the number of features extracted for each audio sample.\n",
        "\n",
        "#### **Additional Features**\n",
        "Consider exploring additional audio features to enhance your model's performance. There are various acoustic properties you could extract from the audio signals, such as zero crossings, harmonic-percussive separation, tempo, spectral centroids, spectral rolloff, chromagram, RMS energy, spectral bandwidth, etc. When working with these features, it's often useful to compute summary statistics like the mean and variance across the audio sample. These summary statistics can capture the overall characteristics and variability of the feature, reducing the dimensionality of your data while retaining important information. Experimenting with these features and their statistical summaries could potentially improve your model's accuracy and robustness in distinguishing between different audio characteristics.\n",
        "\n",
        "#### **Feature Analysis**\n",
        "Don´t forget to optimize the use of features, identifying and handling irrelevant and reduntant features. Then use feature ranking to identify which features are more influential, and evaluate quantitatively how many top-features to retain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc_features(dataset, n_mfcc):\n",
        "    mfcc_features = []\n",
        "\n",
        "    # here we might have used Dataset.map method, unfortunately, it consumes extra memory and runs out of RAM in colab\n",
        "    for entry in tqdm(dataset, desc=\"Extracting MFCC Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "        sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "        mfcc = librosa.feature.mfcc(y=audio_array, sr=sampling_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "        # print(mfcc.shape)\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        mfcc_var = np.var(mfcc, axis=1)\n",
        "\n",
        "        feature_dict = {}\n",
        "\n",
        "        for i in range(n_mfcc):\n",
        "            feature_dict[f'mfcc_mean{i+1}'] = mfcc_mean[i]\n",
        "\n",
        "        for i in range(n_mfcc):\n",
        "            feature_dict[f'mfcc_var{i+1}'] = mfcc_var[i]\n",
        "\n",
        "        feature_dict['genre'] = entry['genre']\n",
        "\n",
        "        mfcc_features.append(feature_dict)\n",
        "\n",
        "    return mfcc_features"
      ],
      "metadata": {
        "id": "1PWpT1RxiHZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKPQSV9gZl2"
      },
      "source": [
        "Let's take a look at the output of the function. We will pass there just 2 samples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXXP3V05gNaN"
      },
      "outputs": [],
      "source": [
        "# extract_mfcc_features(filtered_dataset.select(range(2)), n_mfcc=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oYggNQEkgoN"
      },
      "source": [
        "The function generates `n_mfcc * 2` features for each sample. Consider analyzing their correlation with a matrix and experimenting with different `n_mfcc` values to observe how feature relationships change. While MFCC features are effective for audio analysis, you might also improve performance by incorporating additional features such as RMS or Spectral Contrast. Once you've explored these options, proceed to training the model using the extracted features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTci3PV8zF7D"
      },
      "source": [
        "**Implement functions to extract RMS and Spectral Contrast (10 points in total)**\n",
        "\n",
        "Write functions to extract these features, also add their description and discuss why they might be useful in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In `librosa`, **RMS** and **Spectral Contrast** are essential features for audio analysis, but they measure very different things: one focuses on **power (volume)**, while the other focuses on **texture (clarity)**.\n",
        "\n",
        "\n",
        "**1. Spectral Contrast**\n",
        "\n",
        "Spectral Contrast measures the **spectral texture** of the sound by looking at the \"gap\" between peaks and valleys in the frequency spectrum.\n",
        "\n",
        "* **Definition:** For each frequency sub-band, it calculates the difference between the peaks (high energy) and valleys (low energy).\n",
        "* **Physical Meaning:**\n",
        "  - **High Contrast:** Indicates a \"clear\" sound with distinct harmonics (like a piano or a violin).\n",
        "  - **Low Contrast:** Indicates a \"noisy\" or \"muddy\" sound where the energy is spread flat (like white noise, rain, or a snare drum).\n",
        "\n",
        "\n",
        "**Key Uses:**\n",
        "* **Music Genre Classification:** Jazz and Classical usually have higher contrast than heavy metal or distorted rock.\n",
        "* **Music vs. Speech:** Helping algorithms distinguish between a person talking and an instrument playing.\n",
        "* **Audio Fingerprinting:** Identifying specific sounds based on their unique \"texture.\"\n"
      ],
      "metadata": {
        "id": "TiciMctmCnZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_spectral_contrast(dataset):\n",
        "    spectral_contrast_features = []\n",
        "\n",
        "    for entry in tqdm(dataset, desc=\"Extracting Spectral Contrast Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "        sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "        # Compute spectral contrast\n",
        "        S = np.abs(librosa.stft(audio_array))\n",
        "        spectral_contrast = librosa.feature.spectral_contrast(sr=sampling_rate, S=S)\n",
        "\n",
        "        # print(spectral_contrast.shape)\n",
        "\n",
        "        # Calculate mean and variance\n",
        "        sc_mean = np.mean(spectral_contrast, axis=1)\n",
        "        sc_var = np.var(spectral_contrast, axis=1)\n",
        "\n",
        "        feature_dict = {}\n",
        "        for i in range(len(sc_mean)):\n",
        "            feature_dict[f'spectral_contrast_mean{i+1}'] = sc_mean[i]\n",
        "\n",
        "        for i in range(len(sc_var)):\n",
        "            feature_dict[f'spectral_contrast_var{i+1}'] = sc_var[i]\n",
        "\n",
        "        feature_dict['genre'] = entry['genre']\n",
        "\n",
        "        spectral_contrast_features.append(feature_dict)\n",
        "\n",
        "    return spectral_contrast_features"
      ],
      "metadata": {
        "id": "F3_I5BsriUjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_spectral_contrast(filtered_dataset.select(range(2)))"
      ],
      "metadata": {
        "id": "yCZQF_Dj5dKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. RMS (Root-Mean-Square) Energy**\n",
        "\n",
        "RMS is the standard way to measure the **instantaneous loudness** or energy of an audio signal.\n",
        "\n",
        "* **Definition:** It calculates the square root of the arithmetic mean of the squares of the signal values.\n",
        "* **Physical Meaning:** It represents how \"strong\" the vibrations are. Unlike a simple peak-to-peak measurement, RMS correlates better with how humans perceive volume.\n",
        "\n",
        "**Key Uses:**\n",
        "* **Silence Detection:** Identifying segments of audio where no sound is occurring.\n",
        "* **Audio Normalization:** Adjusting different tracks to have the same average volume.\n",
        "* **Event Detection:** Finding \"hits\" or \"beats\" in a track by looking for sudden rises in energy."
      ],
      "metadata": {
        "id": "PuGWhTYGCvDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_rms(dataset):\n",
        "    rms_features = []\n",
        "\n",
        "    for entry in tqdm(dataset, desc=\"Extracting RMS Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "\n",
        "        rms = librosa.feature.rms(y=audio_array)\n",
        "        # print(rms.shape)\n",
        "        # Calculate mean and variance\n",
        "        rms_mean = np.mean(rms)\n",
        "        rms_var = np.var(rms)\n",
        "\n",
        "        feature_dict = {\n",
        "            'rms_mean': rms_mean,\n",
        "            'rms_var': rms_var,\n",
        "            'genre': entry['genre']\n",
        "        }\n",
        "        rms_features.append(feature_dict)\n",
        "\n",
        "    return rms_features"
      ],
      "metadata": {
        "id": "aIMMb4cviil2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_rms(filtered_dataset.select(range(2)))"
      ],
      "metadata": {
        "id": "xxj2OuvW5iCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug7JJ9Kht8Ou"
      },
      "source": [
        "**Explore and add other features useful for classification (10 points)**\n",
        "\n",
        "It can be in a single function or in separated functions. Select some features and provide a description for each one. You can choose as many features as you like."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero Crossing Rate (ZCR)** is a simple yet powerful feature that describes the \"tonal\" versus \"noisy\" character of a sound.\n",
        "\n",
        "\n",
        "The **Zero Crossing Rate** is the rate at which the signal changes sign—from positive to negative or vice versa—divided by the length of the frame.\n",
        "\n",
        "* **Physical Meaning:** It essentially measures how many times the waveform \"crosses\" the zero-axis.\n",
        "* **Correlation to Frequency:** For a simple sine wave, the ZCR correlates directly to the frequency (pitch). For complex signals, it acts as a proxy for the **dominant frequency** or the \"brightness\" of the sound.\n",
        "\n",
        "\n",
        "**Key Applications**\n",
        "\n",
        "1. **Voiced vs. Unvoiced Speech:**\n",
        "    * **Voiced sounds** (like \"a\", \"e\", \"o\") have low ZCR because they are periodic and lower in frequency.\n",
        "    * **Unvoiced sounds** (like \"s\", \"sh\", \"f\") have high ZCR because they are essentially white noise with high-frequency components.\n",
        "\n",
        "\n",
        "2. **Percussive vs. Harmonic Separation:** It helps distinguish between sharp, noisy hits (high ZCR) and melodic notes (low ZCR).\n",
        "3. **Genre Classification:** Rock or Metal music often has a higher average ZCR compared to Classical music due to distortion and heavy percussion.\n"
      ],
      "metadata": {
        "id": "nte5fX6koe0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_zcr_features(dataset):\n",
        "\n",
        "    zcr_features = []\n",
        "\n",
        "    for entry in tqdm(dataset, desc=\"Extracting ZCR Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "        sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "        # Compute Zero Crossing Rate\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=audio_array)\n",
        "        # print(zcr.shape)\n",
        "        # Calculate mean and variance\n",
        "        zcr_mean = np.mean(zcr)\n",
        "        zcr_var = np.var(zcr)\n",
        "\n",
        "        feature_dict = {\n",
        "            'zcr_mean': zcr_mean,\n",
        "            'zcr_var': zcr_var,\n",
        "            'genre': entry['genre']\n",
        "        }\n",
        "        zcr_features.append(feature_dict)\n",
        "\n",
        "    return zcr_features"
      ],
      "metadata": {
        "id": "WqOYZ_S7ipi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_zcr_features(filtered_dataset.select(range(2)))"
      ],
      "metadata": {
        "id": "cKXUlD8r5ye2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Chroma STFT** (Short-Time Fourier Transform) is a powerful tool used to represent the **harmonic content** of an audio signal. It projects the entire spectrum into 12 bins representing the 12 distinct semitones (or \"chroma\") of the musical octave (C, C\\#, D, D\\#, E, F, F\\#, G, G\\#, A, A\\#, B).\n",
        "\n",
        "\n",
        "Chroma features are based on the concept of **octave equivalence**, meaning that notes separated by one or more octaves are treated as the same. For example, a \"Middle C\" and a \"High C\" are both collapsed into the single \"C\" bin.\n",
        "\n",
        "* **Function:** `librosa.feature.chroma_stft(y=y, sr=sr)`\n",
        "* **Input:** The raw audio or a power spectrogram.\n",
        "* **Output:** A \"Chromagram,\" which shows the intensity of each of the 12 semitones over time.\n",
        "\n",
        "\n",
        "**Key Characteristics**\n",
        "\n",
        "1. **Pitch Class Profile:** It ignores which octave a note is in and focuses strictly on the **pitch class**. This makes it incredibly robust for analyzing melody and harmony.\n",
        "2. **Timbre Invariance:** Because it collapses octaves, Chroma STFT is relatively \"blind\" to the specific instrument playing. A piano playing a C major chord and a guitar playing a C major chord will produce very similar chromagrams.\n",
        "3. **Intensity Mapping:** The values in the chromagram represent the energy present in that specific pitch class at a specific time.\n",
        "\n",
        "\n",
        "**Key Applications**\n",
        "\n",
        "* **Chord Recognition:** Since chords are combinations of specific pitch classes (e.g., C Major is C, E, and G), Chroma STFT is the primary feature used to identify chords in a song.\n",
        "* **Cover Song Identification:** It can identify that two songs are the same melody even if played in different octaves or by different instruments.\n",
        "* **Music Alignment:** It helps sync audio files with MIDI files by matching the harmonic progressions.\n",
        "* **Key Detection:** Analyzing the most dominant chroma bins over a whole track helps determine the musical key (e.g., G Minor).\n",
        "\n"
      ],
      "metadata": {
        "id": "J7s3fFPppABq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_chroma_features(dataset):\n",
        "\n",
        "    chroma_features = []\n",
        "\n",
        "    for entry in tqdm(dataset, desc=\"Extracting Chroma Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "        sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "        # Compute Chromagram\n",
        "        chroma = librosa.feature.chroma_stft(y=audio_array, sr=sampling_rate)\n",
        "        # print(chroma.shape)\n",
        "\n",
        "        # Calculate mean and variance for each chroma bin (12 bins)\n",
        "        chroma_mean = np.mean(chroma, axis=1)\n",
        "        chroma_var = np.var(chroma, axis=1)\n",
        "\n",
        "        feature_dict = {}\n",
        "        for i in range(12):\n",
        "            feature_dict[f'chroma_mean{i+1}'] = chroma_mean[i]\n",
        "        for i in range(12):\n",
        "            feature_dict[f'chroma_var{i+1}'] = chroma_var[i]\n",
        "\n",
        "        feature_dict['genre'] = entry['genre']\n",
        "\n",
        "        chroma_features.append(feature_dict)\n",
        "\n",
        "    return chroma_features"
      ],
      "metadata": {
        "id": "Jnt4Kun6pNNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_chroma_features(filtered_dataset.select(range(2)))"
      ],
      "metadata": {
        "id": "kVUIrM8v53wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature | Category | What it tells the AI |\n",
        "| --- | --- | --- |\n",
        "| **RMS** | Energy | \"How loud is the sound?\" |\n",
        "| **ZCR** | Temporal | \"Is it noisy/hissing or smooth?\" |\n",
        "| **Spectral Contrast** | Spectral | \"Is it a clear note or muddy noise?\" |\n",
        "| **Chroma STFT** | Harmonic | \"What musical notes/chords are being played?\" |\n",
        "| **MFCCs** | Cepstral | \"What is the specific 'texture' or 'voice' of the sound?\" |\n",
        "\n",
        "\n",
        "Other features can be explored in the following link\n",
        "\n",
        "https://librosa.org/doc/latest/feature.html"
      ],
      "metadata": {
        "id": "SeWAvoypvaaT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V41M-1RsuQLC"
      },
      "source": [
        "**(ADV) Analyze correlation of the features (5 points)**\n",
        "\n",
        "Plot correlation diagram and conclude which features are too much correlated and could be removed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_features_list = extract_mfcc_features(filtered_dataset, n_mfcc=8)\n",
        "spectral_contrast_features_list = extract_spectral_contrast(filtered_dataset)\n",
        "rms_features_list = extract_rms(filtered_dataset)\n",
        "zcr_features_list = extract_zcr_features(filtered_dataset)\n",
        "chroma_features_list = extract_chroma_features(filtered_dataset)"
      ],
      "metadata": {
        "id": "KmGNqRo6xruV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists of dictionaries to DataFrames\n",
        "df_mfcc = pd.DataFrame(mfcc_features_list)\n",
        "df_spectral_contrast = pd.DataFrame(spectral_contrast_features_list).drop(columns=['genre'])\n",
        "df_rms = pd.DataFrame(rms_features_list).drop(columns=['genre'])\n",
        "df_zcr = pd.DataFrame(zcr_features_list).drop(columns=['genre'])\n",
        "df_chroma = pd.DataFrame(chroma_features_list).drop(columns=['genre'])\n",
        "\n",
        "# Merge all feature DataFrames\n",
        "# Assuming the order of entries is preserved across extraction functions\n",
        "df0 = pd.concat([\n",
        "    df_mfcc,\n",
        "    df_spectral_contrast,\n",
        "    df_rms,\n",
        "    df_zcr,\n",
        "    df_chroma\n",
        "], axis=1)\n",
        "\n",
        "df = df0.drop(columns=['genre'])\n",
        "corr = df.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# corelation Threshold\n",
        "threshold = 0.9\n",
        "\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = set()\n",
        "pairs = []\n",
        "\n",
        "for col in upper.columns:\n",
        "    # features highly correlated with `col`\n",
        "    high_corr = upper.index[upper[col].abs() > threshold].tolist()\n",
        "    for other in high_corr:\n",
        "        # decide which to drop (here: drop `col`, keep `other`)\n",
        "        to_drop.add(col)\n",
        "        pairs.append((col, other))\n",
        "\n",
        "print(\"Highly correlated pairs (deleted, kept):\")\n",
        "for deleted, kept in pairs:\n",
        "    print(f\"delete: {deleted}  |  keep: {kept}\")\n",
        "\n",
        "to_drop = list(to_drop)\n",
        "print(\"\\nFinal features to delete:\", to_drop)\n",
        "\n",
        "df_reduced = df.drop(columns=to_drop)"
      ],
      "metadata": {
        "id": "FyI3Qqs5Id7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnLnepWN7Weh"
      },
      "source": [
        "**(ADV) Analyze feature importance (5 points)**\n",
        "\n",
        "Analyze the importance of each feature used in the model to understand which variables have the greatest impact on predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree-Based Importance**\n",
        "\n",
        "machine learning (especially for audio classification using the features we discussed), **Tree-Based Importance** refers to techniques used by decision tree algorithms (like **Random Forest** or **XGBoost**) to rank which features were most useful in making a prediction.\n",
        "\n",
        "If you have a dataset containing RMS, ZCR, Chroma, and MFCCs, Tree-Based Importance tells you which of those specific values actually helped the model distinguish between, for example, \"Jazz\" and \"Rock.\"\n",
        "\n",
        "\n",
        "**1. How It Works (The Mechanics)**\n",
        "\n",
        "Decision trees make splits based on features that most effectively decrease **impurity** (usually Gini Impurity or Entropy).\n",
        "\n",
        "* **Gini Importance (Mean Decrease Impurity):** Every time a feature is used to split a node, the algorithm calculates how much the \"purity\" of the samples increased. The more a feature improves purity across all trees in a forest, the higher its importance score.\n",
        "* **Permutation Importance:** A more robust method where a single feature's values are randomly shuffled. If the model's accuracy drops significantly after shuffling \"MFCC-1,\" then MFCC-1 is highly important.\n",
        "\n",
        "\n",
        "\n",
        "**2. Interpreting Importance for Audio**\n",
        "\n",
        "When you run a tree-based model on the librosa features we've covered, you often see patterns like these:\n",
        "\n",
        "* **High MFCC Importance:** Usually means the model is relying on **timbre** (e.g., distinguishing a human voice from a guitar).\n",
        "* **High Chroma Importance:** Usually means the model is relying on **musical keys or harmony** (e.g., distinguishing a Major key pop song from a Minor key blues song).\n",
        "* **High ZCR Importance:** Usually means the model is focusing on **percussive vs. melodic** content (e.g., detecting drum-heavy sections).\n",
        "\n",
        "\n",
        "**3. Why Use It? (Feature Selection)**\n",
        "\n",
        "1. **Dimensionality Reduction:** If your `Chroma_STFT` features all have near-zero importance, you can remove them to make your model faster and less prone to overfitting.\n",
        "2. **Model Explainability:** It allows you to tell a \"story\" about your data (e.g., \"Our AI identifies bird species primarily based on Spectral Contrast and ZCR\")."
      ],
      "metadata": {
        "id": "yFR0hAZKHqQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(df_reduced, df0['genre'])\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:10]  # Top 10\n",
        "print(\"Top Ten Features using TIME based Importance:\",df.columns[indices])"
      ],
      "metadata": {
        "id": "WS4jCbmMtlxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Permutation Importance**\n",
        "\n",
        " **Tree-Based Importance** (Gini Importance) looks at how the model was *built*, **Permutation Importance** looks at how the model *performs* when specific information is taken away.\n",
        "\n",
        "It is often considered more reliable because it is \"model-agnostic\"—you can use it on Random Forests, SVMs, or even Neural Networks.\n",
        "\n",
        "\n",
        "\n",
        "**1. How It Works: The \"Shuffle\" Test**\n",
        "\n",
        "The core idea is to measure how much the model's accuracy drops when you \"break\" a specific feature.\n",
        "\n",
        "1. **Train a model** as you normally would on your audio features (RMS, MFCC, etc.).\n",
        "2. **Record the baseline accuracy** on a validation set.\n",
        "3. **Shuffle one feature:** Take a single column (e.g., `ZCR`) and randomly reorder its values across the rows. This keeps the distribution of the data the same but destroys its relationship with the target label.\n",
        "4. **Re-calculate accuracy:** If the accuracy drops significantly, that feature was **important**. If the accuracy barely changes, the model wasn't really using that feature to make decisions.\n",
        "\n",
        "**2. Why it’s better for Audio Features**\n",
        "\n",
        "In audio analysis, features are often highly correlated. For example, **RMS** and **Spectral Centroid** might both increase during a loud, bright drum hit.\n",
        "\n",
        "* **The Flaw in Gini Importance:** Standard tree-based importance can be biased toward \"high-cardinality\" features (features with many unique floating-point values) like MFCCs, even if they aren't the most predictive.\n",
        "* **The Permutation Advantage:** It reveals if a feature is *actually* providing unique predictive power. If shuffling `MFCC_1` kills your model's performance but shuffling `RMS` does nothing, you know your model is relying on timbre, not volume.\n",
        "\n",
        "**3. Comparison Table**\n",
        "\n",
        "| Feature | Gini Importance (Default Tree) | Permutation Importance |\n",
        "| --- | --- | --- |\n",
        "| **Speed** | Extremely fast (calculated during training). | Slower (requires multiple re-evaluations). |\n",
        "| **Bias** | Biased toward high-cardinality/continuous features. | Unbiased; reflects true predictive power. |\n",
        "| **Data used** | Uses Training Data. | Usually uses Validation/Test Data (shows generalization). |\n",
        "| **Reliability** | Good for a quick glance. | The \"Gold Standard\" for feature selection. |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DizftO-Vt6Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "result = permutation_importance(model, df_reduced, df0['genre'], n_repeats=10, random_state=42)\n",
        "perm_mean = result.importances_mean\n",
        "perm_indices = np.argsort(perm_mean)[::-1]  # Highest importance first\n",
        "top_features = df_reduced.columns[perm_indices][:10]\n",
        "print(\"Top Ten Features using Permutation Importance:\",top_features)"
      ],
      "metadata": {
        "id": "xlpudlkktatC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juTZFIrQusSv"
      },
      "source": [
        "### Train a classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khUd4Trwuy4A"
      },
      "source": [
        "Let's import all necessary functions and classes from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iL_rApSuwcC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uviJSul7jz8s"
      },
      "outputs": [],
      "source": [
        "# include here any additional libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import learning_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSfXqSvvI90"
      },
      "source": [
        "Function to extract features dataset from initial audio dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcM37CK3u-wE"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset,to_drop, n_mfcc):\n",
        "    mfcc_features_list = extract_mfcc_features(dataset, n_mfcc=n_mfcc)\n",
        "\n",
        "    # here you can insert feature extraction functions calls\n",
        "    spectral_contrast_features_list = extract_spectral_contrast(dataset)\n",
        "    rms_features_list = extract_rms(dataset)\n",
        "    zcr_features_list = extract_zcr_features(dataset)\n",
        "    chroma_features_list = extract_chroma_features(dataset)\n",
        "\n",
        "    # Convert lists of dictionaries to DataFrames\n",
        "    df_mfcc = pd.DataFrame(mfcc_features_list)\n",
        "    df_spectral_contrast = pd.DataFrame(spectral_contrast_features_list).drop(columns=['genre'])\n",
        "    df_rms = pd.DataFrame(rms_features_list).drop(columns=['genre'])\n",
        "    df_zcr = pd.DataFrame(zcr_features_list).drop(columns=['genre'])\n",
        "    df_chroma = pd.DataFrame(chroma_features_list).drop(columns=['genre'])\n",
        "\n",
        "    # Merge all feature DataFrames\n",
        "    # Assuming the order of entries is preserved across extraction functions\n",
        "    df = pd.concat([\n",
        "        df_mfcc,\n",
        "        df_spectral_contrast,\n",
        "        df_rms,\n",
        "        df_zcr,\n",
        "        df_chroma\n",
        "    ], axis=1)\n",
        "\n",
        "    # Ensure 'genre' is the target variable\n",
        "    # The 'genre' column from mfcc_features_list is used as it's consistent\n",
        "    df['genre'] = df['genre'].fillna(-1) # Handle missing target values if any\n",
        "\n",
        "    X = df.drop(columns=['genre'])\n",
        "    X = X.drop(columns=to_drop)\n",
        "    y = df['genre']\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuzTWJOyvdZe"
      },
      "source": [
        "Now let's prepare train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26yICQxvEcH"
      },
      "outputs": [],
      "source": [
        "X, y = prepare_dataset(filtered_dataset, to_drop, n_mfcc=8)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHf63HY4JeL6"
      },
      "source": [
        "The best practice is to use a pipeline because it allows us to streamline preprocessing steps (like scaling) and model training into a single workflow. This ensures that all steps are applied consistently during both training and testing, preventing data leakage and simplifying cross-validation and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOxLTs0hu0Sh"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Pipeline\n",
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_lr.fit(X_train, y_train)\n",
        "y_pred = pipeline_lr.predict(X_test)\n",
        "\n",
        "accuracy_lr = accuracy_score(y_test, y_pred)\n",
        "print(f\"{pipeline_lr['classifier'].__class__.__name__}: {accuracy_lr:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaCZPoda29qx"
      },
      "source": [
        "**Train other models and compare their performance (10 points)**\n",
        "\n",
        "Try training different models (at least 3) from sklearn and boosting libraries (e.g. Random Forest, SVM, Gradient Boosting, etc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Nearest Neighbors Pipeline\n",
        "pipeline_knn = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "pipeline_knn.fit(X_train, y_train)\n",
        "y_pred_knn = pipeline_knn.predict(X_test)\n",
        "\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "print(f\"{pipeline_knn['knn'].__class__.__name__} Accuracy Score: {accuracy_knn:.5f}\")"
      ],
      "metadata": {
        "id": "UjB74d-kcsx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Pipeline\n",
        "pipeline_gb = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_gb.fit(X_train, y_train)\n",
        "y_pred_gb = pipeline_gb.predict(X_test)\n",
        "\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"{pipeline_gb['gb'].__class__.__name__} Accuracy Score: {accuracy_gb:.5f}\")"
      ],
      "metadata": {
        "id": "BlnxUWuHcKVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Z1G9le4ipQ"
      },
      "outputs": [],
      "source": [
        "# Random Forest Pipeline\n",
        "pipeline_rf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "y_pred_rf = pipeline_rf.predict(X_test)\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"{pipeline_rf['rf'].__class__.__name__} Accuracy Score: {accuracy_rf:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Pipeline\n",
        "pipeline_svm = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svc', SVC(gamma='auto'))\n",
        "])\n",
        "\n",
        "pipeline_svm.fit(X_train, y_train)\n",
        "y_pred_svm = pipeline_svm.predict(X_test)\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"{pipeline_svm['svc'].__class__.__name__} Accuracy Score: {accuracy_svm:.5f}\")"
      ],
      "metadata": {
        "id": "K2g2uzmEb8f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjN3QMwe3QdE"
      },
      "source": [
        "**(ADV) Choose several best models and perform parametric grid search (10 points)**\n",
        "\n",
        "Choose several of the best-performing models from your previous experiments and tune their hyperparameters using a parametric grid search. Compare the results and discuss which combination performs best."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Logistic Regression,  Random Forest, SVM* are chosen for Grid search"
      ],
      "metadata": {
        "id": "-uuyG5fU64Zh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZtVvJIB4jMc"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression grid search\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'classifier__C': [1,2,3],\n",
        "    'classifier__penalty': ['l1', 'l2'],\n",
        "    'classifier__solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_lr = GridSearchCV(pipeline_lr, param_grid, scoring='accuracy')\n",
        "grid_lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"LogisticRegression grid search:\")\n",
        "print(f\"\\nBest parameters: {grid_lr.best_params_}\")\n",
        "print(\"Best score:\", grid_lr.best_score_)\n",
        "\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_lr = grid_lr.predict(X_test)\n",
        "accuracy_grid_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"\\nAccuracy with grid search: {accuracy_grid_lr:.5f}\")\n",
        "print(f\"Accuracy without grid search: {accuracy_lr:.5f}\")\n",
        "print(f'Improvement in accuracy = {accuracy_grid_lr - accuracy_lr:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest grid search\n",
        "param_grid_rf = {\n",
        "    'rf__n_estimators': [100, 300],\n",
        "    'rf__max_depth': [None, 1,2],\n",
        "    'rf__min_samples_split': [2, 3],\n",
        "    'rf__min_samples_leaf': [1,2,3]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"RandomForestClassifier grid search:\")\n",
        "print(\"\\nBest params:\", grid_rf.best_params_)\n",
        "print(\"Best score:\", grid_rf.best_score_)\n",
        "\n",
        "y_pred_rf = grid_rf.predict(X_test)\n",
        "accuracy_grid_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"\\nAccuracy with grid search: {accuracy_grid_rf:.5f}\")\n",
        "print(f\"Accuracy without grid search: {accuracy_rf:.5f}\")\n",
        "print(f'Improvement in accuracy = {accuracy_grid_rf - accuracy_rf:.5f}')"
      ],
      "metadata": {
        "id": "_BPFBYtokO4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC grid search\n",
        "\n",
        "param_grid_svm = {\n",
        "    'svc__C': [1, 2, 3, 6],\n",
        "    'svc__gamma': [0.015, 0.01, 0.005],\n",
        "    'svc__kernel': ['rbf','sigmoid','poly','linear']\n",
        "}\n",
        "\n",
        "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, scoring='accuracy')\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVC grid search:\")\n",
        "print(\"\\nBest params:\", grid_svm.best_params_)\n",
        "print(\"Best Grid search score:\", grid_svm.best_score_)\n",
        "\n",
        "y_pred_svm = grid_svm.predict(X_test)\n",
        "accuracy_grid_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"\\nAccuracy with grid search: {accuracy_grid_svm:.5f}\")\n",
        "print(f\"Accuracy without grid search: {accuracy_svm:.5f}\")\n",
        "print(f'Improvement in accuracy = {accuracy_grid_svm - accuracy_svm:.5f}')"
      ],
      "metadata": {
        "id": "MTxUG5-ek_V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5lDpCa3bH7"
      },
      "source": [
        "**(ADV) Make the same experiments with some of the best models using cross validation (10 points)**\n",
        "\n",
        "Repeat the experiments for several of the best-performing models using cross-validation. Compare the results with previous single-split evaluations and discuss the stability of model performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using the best models found above\n",
        "best_lr = grid_lr.best_estimator_\n",
        "best_rf = grid_rf.best_estimator_\n",
        "best_svm = grid_svm.best_estimator_\n",
        "\n",
        "# Single Split Score\n",
        "ss_lr = best_lr.score(X_test, y_test)\n",
        "ss_rf = best_rf.score(X_test, y_test)\n",
        "ss_svm = best_svm.score(X_test, y_test)\n",
        "\n",
        "# Cross-Validation Score (Stability Check)\n",
        "cv_lr = cross_val_score(best_lr, X_train, y_train, cv=10)\n",
        "cv_rf = cross_val_score(best_rf, X_train, y_train, cv=10)\n",
        "cv_svm = cross_val_score(best_svm, X_train, y_train, cv=10)\n",
        "\n",
        "# Comparison Table\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression','Random Forest', 'SVM'],\n",
        "    'Single Split': [ss_lr,ss_rf, ss_svm],\n",
        "    'CV Mean': [cv_lr.mean(),cv_rf.mean(), cv_svm.mean()],\n",
        "    'CV Std Dev (Stability)': [cv_lr.std(),cv_rf.std(), cv_svm.std()]\n",
        "})\n",
        "print(results)\n",
        "print(\"the Support Vector Machine (SVM) is the recommended model has hign accuracy and stability\")"
      ],
      "metadata": {
        "id": "vldfRtvFVL3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab6e5656"
      },
      "source": [
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_lr, X_train, y_train, cv=10, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy'\n",
        ")\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Learning Curve (Logistic Regression)\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid()\n",
        "\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca02e778"
      },
      "source": [
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_rf, X_train, y_train, cv=10, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy'\n",
        ")\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Learning Curve (Random Forest)\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid()\n",
        "\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training score is 1 for random forcast, may be overfitting**"
      ],
      "metadata": {
        "id": "LyAiuBXfclxh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ab3381"
      },
      "source": [
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_svm, X_train, y_train, cv=10, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy'\n",
        ")\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Learning Curve (SVM)\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid()\n",
        "\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "         label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "         label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXf1NMCKjuj"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Assignment specifications and rubric\n",
        "\n",
        "**Please use only the provided dataset: no external datasets are allowed.**\n",
        "\n",
        "**The solution notebook MUST run without errors when executing all cells in google colab.**\n",
        "\n",
        "Focus only on classical machine learning algorithms available in *sklearn* for your analysis, other than artificial neural networks.\n",
        "\n",
        "You are welcome to make models based on artificial neural networks too, but these will not be evaluated.\n",
        "\n",
        "You are free to perform additional experiments or analyses beyond those explicitly mentioned in the exercises.\n",
        "\n",
        "Exercises marked **(ADV)** are slightly more advanced and can be completed to earn extra points.\n",
        "To achieve the minimum passing score, it is sufficient to complete only the base-level exercises.\n",
        "\n",
        "### Main steps\n",
        "\n",
        "1. **Data Visualization and Exploration**:\n",
        "   - Visualize and analyze the dataset to gain insights into the distribution and characteristics of different features.\n",
        "\n",
        "2. **Handle Unlabeled and Irrelevant Data**:\n",
        "   - Investigate the dataset for unlabeled data.\n",
        "   - Filter out irrelevant audios, especially those that are just zero signals or contain no meaningful information.\n",
        "\n",
        "3. **Feature Engineering**:\n",
        "   - Experiment with adding new features or refining existing ones.\n",
        "   - Adjust feature extraction parameters to better capture the characteristics of the audio samples.\n",
        "   - Rank the features and assess how many top-features to use.\n",
        "\n",
        "4. **Apply Different Machine Learning Algorithms**:\n",
        "   - Try various machine learning algorithms (e.g., Random Forest, SVM, Gradient Boosting\\*) to improve performance.\n",
        "   - Evaluate the models not only based on the **average accuracy**, but also consider the confusion matrix along with other **per-class** evaluation.\n",
        "   - Explain which metrics are important for evaluating model performance, based on your findings from the exploratory data analysis.\n",
        "\n",
        "\n",
        "\\**Gradient Boosting is commonly implemented using specialized libraries like XGBoost, CatBoost, or LightGBM.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83LKug0A1Hbr"
      },
      "source": [
        "## Assignment Evaluation Criteria (maximum 100%)\n",
        "\n",
        "1. **Data Handling and Preprocessing (5 + 5 = 10 points)**\n",
        "2. **Exploratory Data Analysis (5 + 5 = 10 points)**\n",
        "3. **Feature Engineering (10 + 10 + 5 + 5 = 30 points)**\n",
        "4. **Model Selection and Comparison (10 + 10 + 10 = 30 points)**\n",
        "5. **Clarity, Creativity, and Originality (20 points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsHPvh3ODI33"
      },
      "source": [
        "## Assignment submission instructions\n",
        "Complete the assignment in your own copy of the notebook. Ensure that your notebook is runnable and free of errors. Once finished, test out the notebook in Goggle Colab (to make sure it runs in that environment too). Then RUN-ALL the project and export both the **.ipynb** file and the **html** file. Finally, submit both files via the Teams Assignment section.\n",
        "\n",
        "**The deadline is 14 days ahead of the oral exam.**\n",
        "\n",
        "**Actual deadlines are updated in the Teams Assignment Portal.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d829ba2c"
      },
      "source": [
        "Calculate and plot the learning curves for the Logistic Regression, Random Forest, and Support Vector Machine (SVM) models using their respective best hyperparameters. Analyze these learning curves to understand each model's performance with varying training data sizes, discussing insights into bias, variance, and the impact of more data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}